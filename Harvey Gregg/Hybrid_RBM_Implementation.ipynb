{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdAYEVxg5Bg4"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.7.0\n",
        "!pip install numpy==2.2.6\n",
        "!pip install matplotlib==3.10.3\n",
        "!pip install pennylane==0.41.4\n",
        "!pip install pennylane-lightning[gpu]==0.41.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "OCHUZHGD5OCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "wCqTpIR75sx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sqRBM:\n",
        "    \"\"\"\n",
        "    Semi-quantum Restricted Boltzmann Machine (sqRBM) using a quantum circuit for the hidden layer.\n",
        "\n",
        "    Args:\n",
        "        num_visible (int): Number of visible units.\n",
        "        num_hidden (int): Number of hidden units (equal to number of qubits).\n",
        "        lr (float): Learning rate for Adam optimizer.\n",
        "        k (int): Number of Gibbs sampling steps.\n",
        "        epochs (int): Total training epochs.\n",
        "        gamma (float): Learning rate decay factor.\n",
        "        sigma (float): Std. dev. of Gaussian noise added to visible units during sampling.\n",
        "        batch_size (int): Batch size for training.\n",
        "        weight_decay (float): Weight decay (L2 regularization).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_visible, num_hidden,\n",
        "        lr=1e-3, k=10,\n",
        "        epochs=100, gamma=0.99,\n",
        "        sigma=1e-4, batch_size=300,\n",
        "        weight_decay=1e-4\n",
        "    ):\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lr = lr\n",
        "        self.k = k\n",
        "        self.epochs = epochs\n",
        "        self.gamma = gamma\n",
        "        self.sigma = sigma\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.usedEpochs = []\n",
        "        self.rec_errors = []\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Using device:\", self.device)\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        std = np.sqrt(2.0 / (num_visible + num_hidden))\n",
        "        self.w = torch.randn(num_visible, num_hidden, device=self.device) * std\n",
        "        self.b = torch.zeros(num_visible, device=self.device)\n",
        "        self.c = torch.zeros(num_hidden, device=self.device)\n",
        "\n",
        "        # Persistent chain for CD-k\n",
        "        self.fantasy = torch.randn(batch_size, num_visible, device=self.device)\n",
        "\n",
        "        # Try using GPU-accelerated QPU backend, fallback to CPU\n",
        "        try:\n",
        "            self.dev = qml.device(\"lightning.gpu\", wires=num_hidden)\n",
        "        except:\n",
        "            self.dev = qml.device(\"default.qubit\", wires=num_hidden)\n",
        "            print(\"Falling back to default.qubit (CPU)\")\n",
        "\n",
        "        # Define QNode for quantum expectation over hidden units\n",
        "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"adjoint\")\n",
        "        def circuit(v, w, c):\n",
        "            angles = torch.matmul(v, w) + c\n",
        "            angles = 2.0 * angles  # Optional scaling\n",
        "            for _ in range(2):  # Circuit depth = 2\n",
        "                for i in range(num_hidden):\n",
        "                    qml.RY(angles[i], wires=i)\n",
        "                    qml.RZ(angles[i], wires=i)\n",
        "                for i in range(num_hidden - 1):\n",
        "                    qml.CNOT(wires=[i, i + 1])\n",
        "                if num_hidden > 1:\n",
        "                    qml.CNOT(wires=[num_hidden - 1, 0])\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(num_hidden)]\n",
        "\n",
        "        self.circuit = circuit\n",
        "\n",
        "        # Optimizer and LR scheduler\n",
        "        self.opt = torch.optim.Adam([self.w, self.b, self.c], lr=lr, weight_decay=weight_decay)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=gamma)\n",
        "\n",
        "    def forward(self, v_batch: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply quantum circuit to each data sample in a batch to produce hidden probabilities.\n",
        "\n",
        "        Args:\n",
        "            v_batch (torch.Tensor): Visible batch [batch_size, num_visible].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Hidden probabilities [batch_size, num_hidden].\n",
        "        \"\"\"\n",
        "        v_batch = v_batch.to(self.device).float()\n",
        "        tapes = []\n",
        "\n",
        "        for v in v_batch:\n",
        "            v = v.detach().cpu()\n",
        "            w = self.w.detach().cpu()\n",
        "            c = self.c.detach().cpu()\n",
        "            with qml.tape.QuantumTape() as tape:\n",
        "                angles = torch.matmul(v, w) + c\n",
        "                angles = 2.0 * angles\n",
        "                for _ in range(2):\n",
        "                    for i in range(self.num_hidden):\n",
        "                        qml.RY(angles[i], wires=i)\n",
        "                        qml.RZ(angles[i], wires=i)\n",
        "                    for i in range(self.num_hidden - 1):\n",
        "                        qml.CNOT(wires=[i, i + 1])\n",
        "                    if self.num_hidden > 1:\n",
        "                        qml.CNOT(wires=[self.num_hidden - 1, 0])\n",
        "                for i in range(self.num_hidden):\n",
        "                    qml.expval(qml.PauliZ(i))\n",
        "            tapes.append(tape)\n",
        "\n",
        "        results = qml.execute(tapes, device=self.dev, interface=\"torch\")\n",
        "        results = [torch.tensor(r, dtype=torch.float32) for r in results]\n",
        "        return torch.sigmoid(torch.stack(results).to(self.device))\n",
        "\n",
        "    def sample_visible(self, h_prob: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample visible layer given hidden probabilities.\n",
        "\n",
        "        Args:\n",
        "            h_prob (torch.Tensor): Hidden activations [batch_size, num_hidden].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Sampled visible units.\n",
        "        \"\"\"\n",
        "        h_prob = h_prob.to(self.device)\n",
        "        mean = torch.matmul(h_prob, self.w.T) + self.b\n",
        "        return mean + self.sigma * torch.randn_like(mean)\n",
        "\n",
        "    def fit(self, data: torch.Tensor, epochs: int = None):\n",
        "        \"\"\"\n",
        "        Train the model on a dataset using contrastive divergence.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Input data [N, num_visible].\n",
        "            epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        if epochs is None:\n",
        "            epochs = self.epochs\n",
        "\n",
        "        data = data.clone().detach().float().to(self.device)\n",
        "        n = data.shape[0]\n",
        "\n",
        "        best_error = float('inf')\n",
        "        best_w = self.w.clone()\n",
        "        best_b = self.b.clone()\n",
        "        best_c = self.c.clone()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            perm = torch.randperm(n, device=self.device)\n",
        "            data_shuf = data[perm][:900]  # Truncate to fixed-size sample\n",
        "            epoch_err = 0.0\n",
        "\n",
        "            for i in range(0, len(data_shuf), self.batch_size):\n",
        "                v0 = data_shuf[i : i + self.batch_size]\n",
        "                if v0.size(0) < self.batch_size:\n",
        "                    break\n",
        "\n",
        "                h0 = self.forward(v0)\n",
        "                hk = h0\n",
        "                for _ in range(self.k):\n",
        "                    vk = self.sample_visible(hk)\n",
        "                    hk = self.forward(vk)\n",
        "\n",
        "                # Positive and negative phase\n",
        "                pos = torch.matmul(v0.T, h0)\n",
        "                neg = torch.matmul(vk.T, hk)\n",
        "\n",
        "                # Compute gradients (manual autograd)\n",
        "                self.opt.zero_grad()\n",
        "                self.w.grad = -(pos - neg - 0.001 * self.w)\n",
        "                self.b.grad = -torch.sum(v0 - vk, dim=0)\n",
        "                self.c.grad = -torch.sum(h0 - hk, dim=0)\n",
        "                self.opt.step()\n",
        "\n",
        "                self.scheduler.step()\n",
        "                epoch_err += torch.sum((v0 - vk) ** 2).item() / 900\n",
        "\n",
        "            # Early stopping and weight rollback if error increases\n",
        "            if epoch_err <= best_error:\n",
        "                best_error = epoch_err\n",
        "                best_w = self.w.clone()\n",
        "                best_b = self.b.clone()\n",
        "                best_c = self.c.clone()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    self.w.copy_(best_w)\n",
        "                    self.b.copy_(best_b)\n",
        "                    self.c.copy_(best_c)\n",
        "\n",
        "            self.rec_errors.append(best_error)\n",
        "            self.usedEpochs.append(epoch)\n",
        "            print(f\"Epoch {epoch}/{epochs} â€” Error: {epoch_err:.6f} (Best: {best_error:.6f})\")\n",
        "\n",
        "    def recPlot(self):\n",
        "        \"\"\"\n",
        "        Plot reconstruction error across epochs on a log scale.\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.plot(self.usedEpochs, self.rec_errors, marker='o')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('L2 Reconstruction Error')\n",
        "        plt.title('Reconstruction Error Over Time')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "nrgJPLzG5OX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example Initialisation and Training"
      ],
      "metadata": {
        "id": "o92DxDua6qgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sqRBM5H = sqRBM(30, 5)\n",
        "training_data_pytorch = training_data_pytorch.cpu()\n",
        "sqRBM5H.fit(training_data_pytorch)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Wm3vjK3m63SL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}