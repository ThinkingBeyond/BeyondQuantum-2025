{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIdj0cVA2IMA"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.7.0\n",
        "!pip install scikit-learn==1.6.1\n",
        "!pip install numpy==2.2.6\n",
        "!pip install matplotlib==3.10.3\n",
        "!pip install scipy==1.15.3\n",
        "!pip install pennylane==0.41.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from google.colab import files\n",
        "import csv\n",
        "import pennylane"
      ],
      "metadata": {
        "id": "I6Uvvmr32WTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "365s-zy443AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM(nn.Module):\n",
        "    \"\"\"\n",
        "    Classical Restricted Boltzmann Machine (RBM) with CD or PCD training.\n",
        "\n",
        "    Args:\n",
        "        nv (int): Number of visible units.\n",
        "        nh (int): Number of hidden units.\n",
        "        k (int): Number of Gibbs sampling steps (CD-k).\n",
        "        training (str): Either 'CD' (default) or 'PCD'.\n",
        "        dataset (torch.Tensor, optional): Dataset for training.\n",
        "        visible_normalized (bool): Whether visible units are real values in [0,1].\n",
        "        lr (float): Learning rate.\n",
        "        lr_trend (str): 'linear' (decay to 0) or 'constant'.\n",
        "        bs (int): Batch size.\n",
        "        epochs (int): Number of training epochs.\n",
        "        print_step (int): Frequency of printing loss and metric information.\n",
        "        verbose (bool): Whether to print progress.\n",
        "        track_learning (bool): Whether to track training progress with metrics.\n",
        "        track_method (str): Tracking method, can include 'KDE', 'MMD', 'NDB', or 'rec_error'.\n",
        "        track_step (int): How often to compute tracking metrics.\n",
        "        KDE_bandwidth (float): Optional bandwidth override for KDE tracker.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nv, nh, k=1, training='CD', dataset=None,\n",
        "                 visible_normalized=False, lr=0.001, lr_trend='linear',\n",
        "                 bs=1024, epochs=100, print_step=100, verbose=True,\n",
        "                 track_learning=False, track_method='KDE', track_step=100,\n",
        "                 KDE_bandwidth=None):\n",
        "\n",
        "        super().__init__()\n",
        "        self.nv = nv\n",
        "        self.nh = nh\n",
        "        self.k = k\n",
        "        self.training = training\n",
        "        self.dataset = dataset\n",
        "        self.visible_normalized = visible_normalized\n",
        "        self.lr = lr\n",
        "        self.lr_trend = lr_trend\n",
        "        self.bs = bs\n",
        "        self.epochs = epochs\n",
        "        self.print_step = print_step\n",
        "        self.verbose = verbose\n",
        "        self.track_learning = track_learning\n",
        "        self.track_method = track_method\n",
        "        self.track_step = track_step\n",
        "        self.KDE_bandwidth = KDE_bandwidth\n",
        "\n",
        "        # Model parameters\n",
        "        self.W = nn.Parameter(torch.randn(nh, nv) * 0.01)\n",
        "        self.bv = nn.Parameter(torch.zeros(nv))\n",
        "        self.bh = nn.Parameter(torch.zeros(nh))\n",
        "\n",
        "        self.rec_errs = []  # Track reconstruction error over epochs\n",
        "\n",
        "    def _hidden_prob(self, v):\n",
        "        \"\"\"Compute hidden probabilities p(h=1 | v).\"\"\"\n",
        "        return torch.sigmoid(F.linear(v, self.W, self.bh))\n",
        "\n",
        "    def _sample_hidden(self, v):\n",
        "        \"\"\"Sample hidden units h ~ p(h=1 | v).\"\"\"\n",
        "        p = self._hidden_prob(v)\n",
        "        return torch.bernoulli(p)\n",
        "\n",
        "    def _visible_prob(self, h):\n",
        "        \"\"\"Compute visible probabilities p(v=1 | h).\"\"\"\n",
        "        return torch.sigmoid(F.linear(h, self.W.t(), self.bv))\n",
        "\n",
        "    def _sample_visible(self, h):\n",
        "        \"\"\"Sample visible units v ~ p(v=1 | h).\"\"\"\n",
        "        p = self._visible_prob(h)\n",
        "        if self.visible_normalized:\n",
        "            return p  # Continuous in [0,1]\n",
        "        else:\n",
        "            return torch.bernoulli(p)\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Compute the free energy of visible vector v.\"\"\"\n",
        "        v_term = torch.matmul(v, self.bv)\n",
        "        h_term = torch.sum(F.softplus(F.linear(v, self.W, self.bh)), dim=1)\n",
        "        return -v_term - h_term\n",
        "\n",
        "    def forward(self, v_initial, v_given=None, n_steps=None):\n",
        "        \"\"\"Gibbs sampling starting from v_initial for n_steps.\"\"\"\n",
        "        v = v_initial.clone().detach()\n",
        "        n_steps = n_steps or self.k\n",
        "        for _ in range(n_steps):\n",
        "            h = self._sample_hidden(v)\n",
        "            v = self._sample_visible(h)\n",
        "            if v_given is not None:\n",
        "                v[v_given != -1] = v_given[v_given != -1]\n",
        "        return v\n",
        "\n",
        "    def sample(self, n_samples, v_given=None, therm=10000):\n",
        "        \"\"\"Generate samples from the model.\"\"\"\n",
        "        v = torch.bernoulli(torch.rand(n_samples, self.nv)).to(self.W.device)\n",
        "        if v_given is not None:\n",
        "            v[v_given != -1] = v_given[v_given != -1]\n",
        "        v = self.forward(v, v_given, n_steps=therm)\n",
        "        return v\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Train the RBM on the dataset using CD or PCD.\"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        if self.lr_trend == 'linear':\n",
        "            lr_scheduler = LambdaLR(optimizer, lambda epoch: 1 - (epoch / self.epochs))\n",
        "        else:\n",
        "            lr_scheduler = LambdaLR(optimizer, lambda epoch: 1)\n",
        "\n",
        "        dataset = dataset.to(device)\n",
        "        n = len(dataset)\n",
        "        memory = torch.bernoulli(torch.rand(self.bs, self.nv)).to(device)  # For PCD\n",
        "\n",
        "        if self.track_learning:\n",
        "            track_metrics = get_track_metrics(self.track_method)\n",
        "\n",
        "        for epoch in tqdm(range(1, self.epochs + 1)):\n",
        "            t0 = time.time()\n",
        "            perm = torch.randperm(n)\n",
        "\n",
        "            for i in range(0, n, self.bs):\n",
        "                v0 = dataset[perm[i:i + self.bs]]\n",
        "                v = v0.clone().detach()\n",
        "\n",
        "                if self.training == 'PCD':\n",
        "                    v = self.forward(memory)\n",
        "\n",
        "                # Positive phase\n",
        "                ph0 = self._hidden_prob(v0)\n",
        "                # Negative phase\n",
        "                hk = self._sample_hidden(v)\n",
        "                vk = self._sample_visible(hk)\n",
        "                phk = self._hidden_prob(vk)\n",
        "\n",
        "                # Weight and bias updates\n",
        "                dW = torch.einsum('bi,bj->ij', ph0, v0) - torch.einsum('bi,bj->ij', phk, vk)\n",
        "                dW /= len(v0)\n",
        "\n",
        "                dbv = torch.mean(v0 - vk, dim=0)\n",
        "                dbh = torch.mean(ph0 - phk, dim=0)\n",
        "\n",
        "                self.W.grad = -dW.to(device)\n",
        "                self.bv.grad = -dbv.to(device)\n",
        "                self.bh.grad = -dbh.to(device)\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if self.training == 'PCD':\n",
        "                    memory = vk.detach()\n",
        "\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            if epoch % self.track_step == 0 and self.track_learning:\n",
        "                samples = self.sample(n_samples=len(dataset), therm=1000).detach().cpu()\n",
        "                results = track_metrics(dataset.cpu(), samples, KDE_bandwidth=self.KDE_bandwidth)\n",
        "                if 'rec_error' in results:\n",
        "                    self.rec_errs.append(results['rec_error'])\n",
        "                if self.verbose:\n",
        "                    print(f\"[Epoch {epoch}] \" + \" | \".join([f\"{k}: {v:.4f}\" for k, v in results.items()]))\n",
        "\n",
        "            elif epoch % self.print_step == 0 and self.verbose:\n",
        "                print(f\"[Epoch {epoch}] Time: {time.time() - t0:.1f}s\")\n",
        "\n",
        "    def get_reconstruction_error(self, dataset, n_samples=10000, therm=10000):\n",
        "        \"\"\"\n",
        "        Compute reconstruction error: L2 distance to nearest generated sample.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            samples = self.sample(n_samples=n_samples, therm=therm)\n",
        "            dataset = dataset.to(samples.device)\n",
        "\n",
        "            distances = torch.cdist(dataset.float(), samples.float(), p=2)\n",
        "            min_distances = distances.min(dim=1).values\n",
        "            return torch.mean(min_distances).item()\n",
        "\n",
        "    def check_overfit(self, train_data, val_data):\n",
        "        \"\"\"\n",
        "        Estimate overfitting as free energy difference between train and val sets.\n",
        "        \"\"\"\n",
        "        return self.free_energy(train_data).mean() - self.free_energy(val_data).mean()\n",
        "\n",
        "    def recPlot(self):\n",
        "        \"\"\"Plot reconstruction error over epochs (log scale).\"\"\"\n",
        "        if self.rec_errs:\n",
        "            plt.plot(self.rec_errs)\n",
        "            plt.yscale('log')\n",
        "            plt.xlabel(\"Tracking step\")\n",
        "            plt.ylabel(\"Reconstruction error (L2)\")\n",
        "            plt.title(\"Reconstruction error over training\")\n",
        "            plt.show()\n",
        "\n",
        "    def saveModel(self, filename=None, pickle_protocol=None):\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        if filename is None:\n",
        "            filename = f\"./rbm_nv{self.nv}_nh{self.nh}.pt\"\n",
        "        torch.save(self.state_dict(), filename, pickle_protocol=pickle_protocol)\n"
      ],
      "metadata": {
        "id": "WAcLE46-3Otn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example Initalisation and Training:"
      ],
      "metadata": {
        "id": "iizcTHKz4mAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = []\n",
        "dataset.to('cuda')\n",
        "\n",
        "# number of hidden units\n",
        "nh = 5\n",
        "# number of visible units\n",
        "nv = 30\n",
        "\n",
        "rbm5H = RBM(nv=nv,\n",
        "          nh=nh,\n",
        "          k=2,\n",
        "          training='PCD',\n",
        "          visible_normalized=True,\n",
        "          lr=0.007,\n",
        "          lr_trend='linear',\n",
        "          bs=1000,\n",
        "          epochs=10000,\n",
        "          print_step=100,\n",
        "          verbose=True,\n",
        "          track_method=['rec_error'],\n",
        "          track_learning=True,\n",
        "          track_step = 100)\n",
        "\n",
        "rbm5H.fit(dataset)\n",
        "rbm5H.recPlot()"
      ],
      "metadata": {
        "id": "u783DXX44oaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
